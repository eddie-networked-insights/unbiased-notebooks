{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  2 17:17:32 UTC 2019\r\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  2 17:17:32 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 410.104      Driver Version: 410.104      CUDA Version: 10.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   34C    P0    39W / 300W |      0MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from random import sample\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crawl-300d-2M.vec--> https://fasttext.cc/docs/en/english-vectors.html\n",
    "#When pre-train embedding is helpful? https://www.aclweb.org/anthology/N18-2084\n",
    "#There are many pretrained word embedding models: \n",
    "#fasttext, GloVe, Word2Vec, etc\n",
    "#crawl-300d-2M.vec is trained from Common Crawl (a website that collects almost everything)\n",
    "#it has 2 million words. Each word is represent by a vector of 300 dimensions.\n",
    "\n",
    "#https://nlp.stanford.edu/projects/glove/\n",
    "#GloVe is similar to crawl-300d-2M.vec. Probably, they use different algorithms.\n",
    "#glove.840B.300d.zip: Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download)\n",
    "#tokens mean words. It has 2.2M different words and 840B (likely duplicated) words in total\n",
    "\n",
    "#note that these two pre-trained models give 300d vectors.\n",
    "EMBEDDING_FILES = [\n",
    "    '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec',\n",
    "    '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MODELS = 2\n",
    "# the maximum number of different words to keep in the original texts\n",
    "# 40_000 is a normal number\n",
    "# 100_000 seems good too\n",
    "MAX_FEATURES = 100000 \n",
    "\n",
    "#this is the number of training sample to put in theo model each step\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "#units parameters in Keras.layers.LSTM/cuDNNLSTM\n",
    "#it it the dimension of the output vector of each LSTM cell.\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "\n",
    "EPOCHS = 4\n",
    "\n",
    "#we will convert each word in a comment_text to a number.\n",
    "#So a comment_text is a list of number. How many numbers in this list?\n",
    "#we want the length of this list is a constant -> MAX_LEN\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "\n",
    "def load_embeddings(path):\n",
    "    #each line in the file looks like \n",
    "    # apple 0.3 0.4 0.5 0.6 ...\n",
    "    # that is a word followed by 300 float numbers\n",
    "\n",
    "    with open(path) as f:\n",
    "        #return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
    "        return dict(get_coefs(*o.strip().split(\" \")) for o in tqdm(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(word_index, path):\n",
    "    #path: a path that contains embedding matrix\n",
    "    #word_index is a dict of the form ('apple': 123, 'banana': 349, etc)\n",
    "    # that means word_index[word] gives the index of the word\n",
    "    # word_index was built from all commment_texts\n",
    "\n",
    "    #we will construct an embedding_matrix for the words in word_index\n",
    "    #using pre-trained embedding word vectors from 'path'\n",
    "\n",
    "    embedding_index = load_embeddings(path)\n",
    "\n",
    "    #embedding_matrix is a matrix of len(word_index)+1  x 300\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "\n",
    "    # word_index is a dict. Each element is (word:i) where i is the index\n",
    "    # of the word\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            #RHS is a vector of 300d\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(embedding_matrix, num_aux_targets):\n",
    "   # a simpler version can be found here\n",
    "   # https://www.tensorflow.org/tutorials/keras/basic_text_classification\n",
    "\n",
    "   # Trainable params of the model: 1,671,687\n",
    "   # Recall that the number of samples in train.csv is 1_804_874\n",
    "\n",
    "    #words is a vector of MAX_LEN dimension\n",
    "    words = Input(shape=(MAX_LEN,))\n",
    "\n",
    "    #Embedding is the keras layer. We use the pre-trained embbeding_matrix\n",
    "    # https://keras.io/layers/embeddings/\n",
    "    # have to say that parameters in this layer are not trainable\n",
    "    # x is a vector of 600 dimension\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "\n",
    "    #*embedding_matrix.shape is a short way for \n",
    "    #input_dim = embedding_matrix.shape[0], output_dim  = embedding_matrix.shape[1]\n",
    "\n",
    "    #here the author used pre-train embedding matrix.\n",
    "    #instead of train from begining like in tensorflow example\n",
    "\n",
    "    #https://stackoverflow.com/questions/50393666/how-to-understand-spatialdropout1d-and-when-to-use-it\n",
    "    #x = SpatialDropout1D(0.25)(x)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(1, activation='sigmoid')(hidden)\n",
    "\n",
    "    #num_aux_targets = 6 since y_aux_train has 6 columns\n",
    "    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n",
    "\n",
    "    model = Model(inputs=words, outputs=[result, aux_result])\n",
    "\n",
    "    #model.summary() will gives a good view of the model structure\n",
    "\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(clipnorm=0.1),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take smaller data to use for prototyping\n",
    "total_dataset_size = 6000\n",
    "test_size = 1000\n",
    "data_sample = train.sample(total_dataset_size, random_state=1)\n",
    "del train\n",
    "del test\n",
    "train, test = train_test_split(data_sample, train_size=total_dataset_size-test_size, \n",
    "                               test_size=test_size, random_state=1, shuffle=True)\n",
    "del data_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inspect data_sample etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>467525</th>\n",
       "      <td>817578</td>\n",
       "      <td>0.3</td>\n",
       "      <td>'The real health-care change we need? Strong l...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>161016</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243616</th>\n",
       "      <td>5634256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"It’s in the entire state’s best interest to b...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>357823</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426271</th>\n",
       "      <td>765291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>In case you haven't seen it, they showed this ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>158952</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085322</th>\n",
       "      <td>5443027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Condolences to the families, friends and commu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>345906</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621828</th>\n",
       "      <td>1003344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Agreed although the suggested amount of compen...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>168555</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  target                                       comment_text  \\\n",
       "467525    817578     0.3  'The real health-care change we need? Strong l...   \n",
       "1243616  5634256     0.0  \"It’s in the entire state’s best interest to b...   \n",
       "426271    765291     0.0  In case you haven't seen it, they showed this ...   \n",
       "1085322  5443027     0.0  Condolences to the families, friends and commu...   \n",
       "621828   1003344     0.0  Agreed although the suggested amount of compen...   \n",
       "\n",
       "         severe_toxicity  obscene  identity_attack  insult  threat  asian  \\\n",
       "467525               0.1      0.0              0.0     0.3     0.0    NaN   \n",
       "1243616              0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "426271               0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "1085322              0.0      0.0              0.0     0.0     0.0    0.0   \n",
       "621828               0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "\n",
       "         atheist  ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
       "467525       NaN  ...      161016  rejected      0    0    1      1         0   \n",
       "1243616      NaN  ...      357823  approved      0    0    2      8         0   \n",
       "426271       NaN  ...      158952  approved      0    0    0      2         0   \n",
       "1085322      0.0  ...      345906  approved      0    0    0      0         0   \n",
       "621828       NaN  ...      168555  approved      0    0    0      1         0   \n",
       "\n",
       "         sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
       "467525               0.0                         0                        10  \n",
       "1243616              0.0                         0                         4  \n",
       "426271               0.0                         0                         4  \n",
       "1085322              0.0                        10                         4  \n",
       "621828               0.0                         0                         4  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#Take the columns 'comment_text' from train,\n",
    "# then fillall NaN values by emtpy string '' (redundant)\n",
    "x_train = train['comment_text'].fillna('').values\n",
    "\n",
    "#if true, y_train[i] =1, if false, it is = 0\n",
    "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
    "\n",
    "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "\n",
    "#\n",
    "#Take the columns 'comment_text' from test,\n",
    "# then fillall NaN values by emtpy string '' (redundant)\n",
    "x_test = test['comment_text'].fillna('').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eddie added\n",
    "y_test = np.where(test['target'] >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/preprocessing/text/\n",
    "# tokenizer is a class with some method\n",
    "tokenizer = text.Tokenizer(num_words=MAX_FEATURES)\n",
    "\n",
    "#we apply method fit_on_texts of tokenizer on x_train and x_test\n",
    "#it will initialize some parameters/attribute inside tokenizer\n",
    "#https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/text.py#L139\n",
    "#https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/text.py#L210\n",
    "\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test)) #QUESTION TODO remove x_test?\n",
    "#for example, after fit_on_texts, we can type\n",
    "#tokenizer.word_counts #give a OderedDict\n",
    "#tokenizer.document_counts # an int\n",
    "#tokenizer.word_index is a dict of words with correponding indices\n",
    "#There are 410046 different words in all 'comment_text'\n",
    "#len(tokenizer.word_index) == 410_046\n",
    "\n",
    "\n",
    "#these words come from all 'comment_text' in training.csv and test.csv\n",
    "\n",
    "#tokenizer.index_word: the inverse of tokenizer.word_index\n",
    "\n",
    "\n",
    "#https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/text.py#L267\n",
    "#we will convert each word in a comment_text to a number.\n",
    "#So a comment_text is a list of number.\n",
    "\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "\n",
    "#https://keras.io/preprocessing/sequence/\n",
    "# https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/sequence.py\n",
    "#each comment_text is now a list of word\n",
    "# we want the length of this list is a constant -> MAX_LEN\n",
    "# if the list is longer, then we cut/trim it \n",
    "# if shorter, then we add/pad it with 0's at the beginning\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000001it [02:14, 14880.03it/s]\n",
      "2196018it [02:36, 14057.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# create an embedding_matrix \n",
    "#after this, embedding_matrix is a matrix of size\n",
    "# len(tokenizer.word_index)+1   x 600\n",
    "# we concatenate two matrices, 600 = 300+300\n",
    "embedding_matrix = np.concatenate(\n",
    "    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n",
    "#embedding_matrix.shape \n",
    "#== (410047, 600)\n",
    "\n",
    "#embedding_matrix[i] is a 600d vector representation of the word whose index is i\n",
    "#embedding_matrix[10]\n",
    "#tokenizer.index_word[10] == 'you'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample of train dataset for prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rand_indices = sample(range(len(x_train)),total_dataset_size)\n",
    "# data_sample_x = x_train[rand_indices[0]:rand_indices[0]+1]\n",
    "# data_sample_y = y_train[rand_indices[0]:rand_indices[0]+1]\n",
    "# for i in range(1,len(rand_indices)):\n",
    "#     data_sample_x = np.concatenate((data_sample_x,x_train[rand_indices[i]:rand_indices[i]+1]),\n",
    "#                                    axis=0)\n",
    "#     data_sample_y = np.concatenate((data_sample_y,y_train[rand_indices[i]:rand_indices[i]+1]),\n",
    "#                                    axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(5, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 26s 7ms/step - loss: 0.6478 - dense_139_loss: 0.3383 - dense_140_loss: 0.3096 - dense_139_acc: 0.9230 - dense_140_acc: 0.8135\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 286us/step - loss: 0.4223 - dense_139_loss: 0.2683 - dense_140_loss: 0.1540 - dense_139_acc: 0.9230 - dense_140_acc: 0.8557\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 277us/step - loss: 0.4013 - dense_139_loss: 0.2573 - dense_140_loss: 0.1440 - dense_139_acc: 0.9230 - dense_140_acc: 0.8557\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 280us/step - loss: 0.3912 - dense_139_loss: 0.2500 - dense_140_loss: 0.1412 - dense_139_acc: 0.9230 - dense_140_acc: 0.8557\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 27s 7ms/step - loss: 0.7205 - dense_143_loss: 0.3787 - dense_144_loss: 0.3418 - dense_143_acc: 0.8157 - dense_144_acc: 0.7915\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 283us/step - loss: 0.4362 - dense_143_loss: 0.2823 - dense_144_loss: 0.1539 - dense_143_acc: 0.9230 - dense_144_acc: 0.8557\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 284us/step - loss: 0.4152 - dense_143_loss: 0.2688 - dense_144_loss: 0.1464 - dense_143_acc: 0.9230 - dense_144_acc: 0.8557\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 280us/step - loss: 0.4094 - dense_143_loss: 0.2634 - dense_144_loss: 0.1461 - dense_143_acc: 0.9230 - dense_144_acc: 0.8557\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 28s 7ms/step - loss: 0.6674 - dense_147_loss: 0.3577 - dense_148_loss: 0.3097 - dense_147_acc: 0.9165 - dense_148_acc: 0.8090\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 281us/step - loss: 0.4331 - dense_147_loss: 0.2755 - dense_148_loss: 0.1576 - dense_147_acc: 0.9225 - dense_148_acc: 0.8536\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 277us/step - loss: 0.4135 - dense_147_loss: 0.2660 - dense_148_loss: 0.1476 - dense_147_acc: 0.9225 - dense_148_acc: 0.8536\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 286us/step - loss: 0.4013 - dense_147_loss: 0.2573 - dense_148_loss: 0.1440 - dense_147_acc: 0.9225 - dense_148_acc: 0.8536\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 29s 7ms/step - loss: 0.6656 - dense_151_loss: 0.3584 - dense_152_loss: 0.3073 - dense_151_acc: 0.9128 - dense_152_acc: 0.7948\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 277us/step - loss: 0.4234 - dense_151_loss: 0.2724 - dense_152_loss: 0.1509 - dense_151_acc: 0.9225 - dense_152_acc: 0.8536\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 277us/step - loss: 0.4133 - dense_151_loss: 0.2673 - dense_152_loss: 0.1460 - dense_151_acc: 0.9225 - dense_152_acc: 0.8536\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 277us/step - loss: 0.3965 - dense_151_loss: 0.2538 - dense_152_loss: 0.1427 - dense_151_acc: 0.9225 - dense_152_acc: 0.8536\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 30s 8ms/step - loss: 0.6709 - dense_155_loss: 0.3705 - dense_156_loss: 0.3004 - dense_155_acc: 0.8558 - dense_156_acc: 0.8310\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 275us/step - loss: 0.4217 - dense_155_loss: 0.2707 - dense_156_loss: 0.1511 - dense_155_acc: 0.9262 - dense_156_acc: 0.8575\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 276us/step - loss: 0.3967 - dense_155_loss: 0.2549 - dense_156_loss: 0.1418 - dense_155_acc: 0.9262 - dense_156_acc: 0.8575\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 284us/step - loss: 0.3773 - dense_155_loss: 0.2405 - dense_156_loss: 0.1368 - dense_155_acc: 0.9263 - dense_156_acc: 0.8575\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 32s 8ms/step - loss: 0.6677 - dense_159_loss: 0.3539 - dense_160_loss: 0.3138 - dense_159_acc: 0.8295 - dense_160_acc: 0.7904\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 283us/step - loss: 0.4128 - dense_159_loss: 0.2644 - dense_160_loss: 0.1484 - dense_159_acc: 0.9263 - dense_160_acc: 0.8575\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 282us/step - loss: 0.4027 - dense_159_loss: 0.2604 - dense_160_loss: 0.1424 - dense_159_acc: 0.9263 - dense_160_acc: 0.8575\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 283us/step - loss: 0.3929 - dense_159_loss: 0.2529 - dense_160_loss: 0.1400 - dense_159_acc: 0.9262 - dense_160_acc: 0.8575\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 32s 8ms/step - loss: 0.6480 - dense_163_loss: 0.3498 - dense_164_loss: 0.2982 - dense_163_acc: 0.9257 - dense_164_acc: 0.7918\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 277us/step - loss: 0.4239 - dense_163_loss: 0.2705 - dense_164_loss: 0.1534 - dense_163_acc: 0.9265 - dense_164_acc: 0.8562\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 279us/step - loss: 0.4085 - dense_163_loss: 0.2646 - dense_164_loss: 0.1439 - dense_163_acc: 0.9265 - dense_164_acc: 0.8562\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 282us/step - loss: 0.3992 - dense_163_loss: 0.2568 - dense_164_loss: 0.1424 - dense_163_acc: 0.9265 - dense_164_acc: 0.8562\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 33s 8ms/step - loss: 0.6217 - dense_167_loss: 0.3201 - dense_168_loss: 0.3016 - dense_167_acc: 0.9265 - dense_168_acc: 0.7932\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 281us/step - loss: 0.4237 - dense_167_loss: 0.2731 - dense_168_loss: 0.1506 - dense_167_acc: 0.9265 - dense_168_acc: 0.8562\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 281us/step - loss: 0.4042 - dense_167_loss: 0.2602 - dense_168_loss: 0.1439 - dense_167_acc: 0.9265 - dense_168_acc: 0.8562\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 278us/step - loss: 0.3970 - dense_167_loss: 0.2552 - dense_168_loss: 0.1419 - dense_167_acc: 0.9265 - dense_168_acc: 0.8562\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 34s 9ms/step - loss: 0.6746 - dense_171_loss: 0.3639 - dense_172_loss: 0.3107 - dense_171_acc: 0.8792 - dense_172_acc: 0.8049\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 282us/step - loss: 0.4381 - dense_171_loss: 0.2839 - dense_172_loss: 0.1542 - dense_171_acc: 0.9187 - dense_172_acc: 0.8514\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 279us/step - loss: 0.4188 - dense_171_loss: 0.2713 - dense_172_loss: 0.1475 - dense_171_acc: 0.9187 - dense_172_acc: 0.8514\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 279us/step - loss: 0.3983 - dense_171_loss: 0.2562 - dense_172_loss: 0.1421 - dense_171_acc: 0.9188 - dense_172_acc: 0.8514\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 35s 9ms/step - loss: 0.7485 - dense_175_loss: 0.4107 - dense_176_loss: 0.3378 - dense_175_acc: 0.8113 - dense_176_acc: 0.7730\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 283us/step - loss: 0.4590 - dense_175_loss: 0.2974 - dense_176_loss: 0.1616 - dense_175_acc: 0.9188 - dense_176_acc: 0.8514\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 278us/step - loss: 0.4328 - dense_175_loss: 0.2822 - dense_176_loss: 0.1506 - dense_175_acc: 0.9188 - dense_176_acc: 0.8514\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 281us/step - loss: 0.4248 - dense_175_loss: 0.2770 - dense_176_loss: 0.1478 - dense_175_acc: 0.9187 - dense_176_acc: 0.8514\n",
      "CPU times: user 11min 48s, sys: 31.1 s, total: 12min 19s\n",
      "Wall time: 11min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_acc = []\n",
    "val_auroc1 = []\n",
    "val_f1 = []\n",
    "val_bin_cross = []\n",
    "# enumerate splits\n",
    "for i_train, i_test in kfold.split(x_train):\n",
    "    #print('train: %s, test: %s' % (data[i_train], data[i_test]))\n",
    "\n",
    "    model = None\n",
    "    checkpoint_predictions = []\n",
    "    weights = []\n",
    "\n",
    "\n",
    "\n",
    "    #https://keras.io/callbacks/#learningratescheduler\n",
    "\n",
    "    for model_idx in range(NUM_MODELS):\n",
    "      # build the same models\n",
    "        model = build_model(embedding_matrix, y_aux_train.shape[-1])\n",
    "      # We train each model EPOCHS times\n",
    "      # After each epoch, we reset learning rate (we are using Adam Optimizer)  \n",
    "      # https://towardsdatascience.com/learning-rate-scheduler-d8a55747dd90\n",
    "\n",
    "      # https://github.com/keras-team/keras/blob/master/keras/callbacks.py#L921\n",
    "      # learningrate is the attribute 'lr' from Adam optimizer\n",
    "      # see https://github.com/keras-team/keras/blob/master/keras/optimizers.py#L460\n",
    "      # In Adam Optimizer, learning rate is changing after each batch\n",
    "        for global_epoch in range(EPOCHS):\n",
    "            model.fit(\n",
    "                x_train[i_train], #2019JUL1 is i_train the spread out indices of the kfold?\n",
    "                [y_train[i_train], y_aux_train.iloc[i_train,:]],\n",
    "                batch_size=BATCH_SIZE,\n",
    "                epochs=1,\n",
    "                verbose=1,\n",
    "                callbacks=[\n",
    "                    #LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch), verbose = 1)\n",
    "                    LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n",
    "                ]#,\n",
    "                #validation=[,y_test[i_test]]  #2019JUL1: is this right?\n",
    "            )\n",
    "            #model.predict will give two outputs: main_output (target) and aux_output\n",
    "            #we only take main_output\n",
    "            checkpoint_predictions.append(model.predict(x_train[i_test], batch_size=2048)[0].flatten()) #is the normal way of \n",
    "                # of validating incompatible with this kernel because test is used in every iteration? maybe this is \n",
    "                # something to discuss with the team.\n",
    "                # Change the training completely to something more normal? see how that performs?\n",
    "                # Just ignore the checkpoints and weights and use the final model at the end for prediction on x_test?\n",
    "                # Remove the learning rate scheduler?\n",
    "            weights.append(2 ** global_epoch)\n",
    "    predictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n",
    "    \n",
    "    val_bin_cross.append(log_loss(y_train[i_test],predictions))\n",
    "    \n",
    "    #val_f1.append(f1_score(y_train[i_test],predictions))  #currently there is some bug\n",
    "    \n",
    "    #roc_auc\n",
    "    val_auroc1.append(roc_auc_score(y_train[i_test],predictions))\n",
    "    \n",
    "    #accuracy\n",
    "    predictions = np.where(predictions >= 0.5, 1, 0)\n",
    "    val_acc.append(accuracy_score(y_train[i_test], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9234\n",
      "0.7679749474533772\n",
      "0.2564531115626966\n"
     ]
    }
   ],
   "source": [
    "#final score (average) of kfold cv of ensemble model\n",
    "print(sum(val_acc)/float(len(val_acc)))\n",
    "\n",
    "print(sum(val_auroc1)/float(len(val_auroc1)))\n",
    "#print(sum(val_f1)/float(len(val_f1)))\n",
    "print(sum(val_bin_cross)/float(len(val_bin_cross)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.9315999999999999\n",
    "0.8963434947479014\n",
    "0.18568469874299043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7751783783783784,\n",
       " 0.7841763827932201,\n",
       " 0.7638681220095694,\n",
       " 0.7717904759555495,\n",
       " 0.7329965590453181]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat the cell above for another model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "4000/4000 [==============================] - 25s 6ms/step - loss: 0.6765 - dense_179_loss: 0.3540 - dense_180_loss: 0.3225 - dense_179_acc: 0.9227 - dense_180_acc: 0.7762\n",
      "Epoch 2/8\n",
      "4000/4000 [==============================] - 1s 281us/step - loss: 0.4396 - dense_179_loss: 0.2843 - dense_180_loss: 0.1553 - dense_179_acc: 0.9230 - dense_180_acc: 0.8557\n",
      "Epoch 3/8\n",
      "4000/4000 [==============================] - 1s 277us/step - loss: 0.4097 - dense_179_loss: 0.2631 - dense_180_loss: 0.1466 - dense_179_acc: 0.9230 - dense_180_acc: 0.8557\n",
      "Epoch 4/8\n",
      "4000/4000 [==============================] - 1s 274us/step - loss: 0.3716 - dense_179_loss: 0.2340 - dense_180_loss: 0.1376 - dense_179_acc: 0.9230 - dense_180_acc: 0.8557\n",
      "Epoch 5/8\n",
      "4000/4000 [==============================] - 1s 281us/step - loss: 0.3550 - dense_179_loss: 0.2196 - dense_180_loss: 0.1354 - dense_179_acc: 0.9243 - dense_180_acc: 0.8554\n",
      "Epoch 6/8\n",
      "4000/4000 [==============================] - 1s 282us/step - loss: 0.3176 - dense_179_loss: 0.1921 - dense_180_loss: 0.1255 - dense_179_acc: 0.9222 - dense_180_acc: 0.8557\n",
      "Epoch 7/8\n",
      "4000/4000 [==============================] - 1s 288us/step - loss: 0.2995 - dense_179_loss: 0.1762 - dense_180_loss: 0.1233 - dense_179_acc: 0.9260 - dense_180_acc: 0.8550\n",
      "Epoch 8/8\n",
      "4000/4000 [==============================] - 1s 280us/step - loss: 0.2850 - dense_179_loss: 0.1643 - dense_180_loss: 0.1207 - dense_179_acc: 0.9367 - dense_180_acc: 0.8552\n",
      "Epoch 1/8\n",
      "4000/4000 [==============================] - 24s 6ms/step - loss: 0.7217 - dense_183_loss: 0.3691 - dense_184_loss: 0.3526 - dense_183_acc: 0.8235 - dense_184_acc: 0.7518\n",
      "Epoch 2/8\n",
      "4000/4000 [==============================] - 1s 281us/step - loss: 0.4287 - dense_183_loss: 0.2724 - dense_184_loss: 0.1563 - dense_183_acc: 0.9225 - dense_184_acc: 0.8536\n",
      "Epoch 3/8\n",
      "4000/4000 [==============================] - 1s 278us/step - loss: 0.4063 - dense_183_loss: 0.2622 - dense_184_loss: 0.1441 - dense_183_acc: 0.9225 - dense_184_acc: 0.8536\n",
      "Epoch 4/8\n",
      "4000/4000 [==============================] - 1s 282us/step - loss: 0.3710 - dense_183_loss: 0.2331 - dense_184_loss: 0.1379 - dense_183_acc: 0.9230 - dense_184_acc: 0.8536\n",
      "Epoch 5/8\n",
      "4000/4000 [==============================] - 1s 275us/step - loss: 0.3369 - dense_183_loss: 0.2065 - dense_184_loss: 0.1304 - dense_183_acc: 0.9263 - dense_184_acc: 0.8536\n",
      "Epoch 6/8\n",
      "4000/4000 [==============================] - 1s 285us/step - loss: 0.3337 - dense_183_loss: 0.1988 - dense_184_loss: 0.1349 - dense_183_acc: 0.9295 - dense_184_acc: 0.8535\n",
      "Epoch 7/8\n",
      "4000/4000 [==============================] - 1s 281us/step - loss: 0.3189 - dense_183_loss: 0.1918 - dense_184_loss: 0.1270 - dense_183_acc: 0.9257 - dense_184_acc: 0.8532\n",
      "Epoch 8/8\n",
      "4000/4000 [==============================] - 1s 282us/step - loss: 0.3124 - dense_183_loss: 0.1835 - dense_184_loss: 0.1289 - dense_183_acc: 0.9342 - dense_184_acc: 0.8534\n",
      "Epoch 1/8\n",
      "4000/4000 [==============================] - 27s 7ms/step - loss: 0.6564 - dense_187_loss: 0.3318 - dense_188_loss: 0.3247 - dense_187_acc: 0.9258 - dense_188_acc: 0.7771\n",
      "Epoch 2/8\n",
      "4000/4000 [==============================] - 1s 280us/step - loss: 0.4254 - dense_187_loss: 0.2745 - dense_188_loss: 0.1508 - dense_187_acc: 0.9262 - dense_188_acc: 0.8575\n",
      "Epoch 3/8\n",
      "4000/4000 [==============================] - 1s 285us/step - loss: 0.3992 - dense_187_loss: 0.2570 - dense_188_loss: 0.1422 - dense_187_acc: 0.9262 - dense_188_acc: 0.8575\n",
      "Epoch 4/8\n",
      "4000/4000 [==============================] - 1s 279us/step - loss: 0.3584 - dense_187_loss: 0.2240 - dense_188_loss: 0.1343 - dense_187_acc: 0.9265 - dense_188_acc: 0.8575\n",
      "Epoch 5/8\n",
      "4000/4000 [==============================] - 1s 283us/step - loss: 0.3304 - dense_187_loss: 0.2012 - dense_188_loss: 0.1292 - dense_187_acc: 0.9277 - dense_188_acc: 0.8575\n",
      "Epoch 6/8\n",
      "4000/4000 [==============================] - 1s 282us/step - loss: 0.2992 - dense_187_loss: 0.1768 - dense_188_loss: 0.1224 - dense_187_acc: 0.9368 - dense_188_acc: 0.8575\n",
      "Epoch 7/8\n",
      "4000/4000 [==============================] - 1s 286us/step - loss: 0.2897 - dense_187_loss: 0.1676 - dense_188_loss: 0.1221 - dense_187_acc: 0.9397 - dense_188_acc: 0.8575\n",
      "Epoch 8/8\n",
      "4000/4000 [==============================] - 1s 281us/step - loss: 0.2678 - dense_187_loss: 0.1492 - dense_188_loss: 0.1186 - dense_187_acc: 0.9425 - dense_188_acc: 0.8573\n",
      "Epoch 1/8\n",
      "4000/4000 [==============================] - 25s 6ms/step - loss: 0.7033 - dense_191_loss: 0.3578 - dense_192_loss: 0.3455 - dense_191_acc: 0.8607 - dense_192_acc: 0.7451\n",
      "Epoch 2/8\n",
      "4000/4000 [==============================] - 1s 280us/step - loss: 0.4246 - dense_191_loss: 0.2722 - dense_192_loss: 0.1524 - dense_191_acc: 0.9265 - dense_192_acc: 0.8562\n",
      "Epoch 3/8\n",
      "4000/4000 [==============================] - 1s 283us/step - loss: 0.4034 - dense_191_loss: 0.2590 - dense_192_loss: 0.1444 - dense_191_acc: 0.9265 - dense_192_acc: 0.8562\n",
      "Epoch 4/8\n",
      "4000/4000 [==============================] - 1s 279us/step - loss: 0.3821 - dense_191_loss: 0.2433 - dense_192_loss: 0.1387 - dense_191_acc: 0.9265 - dense_192_acc: 0.8562\n",
      "Epoch 5/8\n",
      "4000/4000 [==============================] - 1s 280us/step - loss: 0.3399 - dense_191_loss: 0.2087 - dense_192_loss: 0.1312 - dense_191_acc: 0.9267 - dense_192_acc: 0.8562\n",
      "Epoch 6/8\n",
      "4000/4000 [==============================] - 1s 279us/step - loss: 0.3136 - dense_191_loss: 0.1873 - dense_192_loss: 0.1263 - dense_191_acc: 0.9305 - dense_192_acc: 0.8561\n",
      "Epoch 7/8\n",
      "4000/4000 [==============================] - 1s 281us/step - loss: 0.2990 - dense_191_loss: 0.1747 - dense_192_loss: 0.1244 - dense_191_acc: 0.9328 - dense_192_acc: 0.8560\n",
      "Epoch 8/8\n",
      "4000/4000 [==============================] - 1s 280us/step - loss: 0.2764 - dense_191_loss: 0.1550 - dense_192_loss: 0.1214 - dense_191_acc: 0.9375 - dense_192_acc: 0.8558\n",
      "Epoch 1/8\n",
      "4000/4000 [==============================] - 26s 6ms/step - loss: 0.7424 - dense_195_loss: 0.3939 - dense_196_loss: 0.3486 - dense_195_acc: 0.8115 - dense_196_acc: 0.7676\n",
      "Epoch 2/8\n",
      "4000/4000 [==============================] - 1s 277us/step - loss: 0.4499 - dense_195_loss: 0.2917 - dense_196_loss: 0.1581 - dense_195_acc: 0.9188 - dense_196_acc: 0.8514\n",
      "Epoch 3/8\n",
      "4000/4000 [==============================] - 1s 276us/step - loss: 0.4355 - dense_195_loss: 0.2848 - dense_196_loss: 0.1507 - dense_195_acc: 0.9188 - dense_196_acc: 0.8514\n",
      "Epoch 4/8\n",
      "4000/4000 [==============================] - 1s 278us/step - loss: 0.3900 - dense_195_loss: 0.2482 - dense_196_loss: 0.1419 - dense_195_acc: 0.9190 - dense_196_acc: 0.8514\n",
      "Epoch 5/8\n",
      "4000/4000 [==============================] - 1s 276us/step - loss: 0.3718 - dense_195_loss: 0.2341 - dense_196_loss: 0.1378 - dense_195_acc: 0.9128 - dense_196_acc: 0.8513\n",
      "Epoch 6/8\n",
      "4000/4000 [==============================] - 1s 282us/step - loss: 0.3349 - dense_195_loss: 0.2040 - dense_196_loss: 0.1309 - dense_195_acc: 0.9245 - dense_196_acc: 0.8513\n",
      "Epoch 7/8\n",
      "4000/4000 [==============================] - 1s 279us/step - loss: 0.3126 - dense_195_loss: 0.1859 - dense_196_loss: 0.1267 - dense_195_acc: 0.9305 - dense_196_acc: 0.8514\n",
      "Epoch 8/8\n",
      "4000/4000 [==============================] - 1s 278us/step - loss: 0.3016 - dense_195_loss: 0.1753 - dense_196_loss: 0.1262 - dense_195_acc: 0.9312 - dense_196_acc: 0.8513\n",
      "CPU times: user 6min 27s, sys: 15.5 s, total: 6min 42s\n",
      "Wall time: 6min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_acc = []\n",
    "val_auroc2 = []\n",
    "val_f1 = []\n",
    "val_bin_cross = []\n",
    "# enumerate splits\n",
    "for i_train, i_test in kfold.split(x_train):\n",
    "    #print('train: %s, test: %s' % (data[i_train], data[i_test]))\n",
    "\n",
    "    model = None\n",
    "    checkpoint_predictions = []\n",
    "    weights = []\n",
    "\n",
    "    \n",
    "    model = build_model(embedding_matrix, y_aux_train.shape[-1])\n",
    "    model.fit(\n",
    "        x_train[i_train], #2019JUL1 is i_train the spread out indices of the kfold?\n",
    "        [y_train[i_train], y_aux_train.iloc[i_train,:]],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=8,\n",
    "        verbose=1\n",
    "    )\n",
    "    predictions = model.predict(x_train[i_test], batch_size=2048)[0].flatten()\n",
    "    \n",
    "    \n",
    "    \n",
    "    val_bin_cross.append(log_loss(y_train[i_test],predictions))\n",
    "    \n",
    "    #val_f1.append(f1_score(y_train[i_test],predictions))  #currently there is some bug\n",
    "    \n",
    "    #roc_auc\n",
    "    val_auroc2.append(roc_auc_score(y_train[i_test],predictions))\n",
    "    \n",
    "    #accuracy\n",
    "    predictions = np.where(predictions >= 0.5, 1, 0)\n",
    "    val_acc.append(accuracy_score(y_train[i_test], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9315999999999999\n",
      "0.8963434947479014\n",
      "0.18568469874299043\n"
     ]
    }
   ],
   "source": [
    "#final score (average) of kfold cv of ensemble model\n",
    "print(sum(val_acc)/float(len(val_acc)))  #high is good\n",
    "\n",
    "print(sum(val_auroc2)/float(len(val_auroc2)))  #high is good\n",
    "#print(sum(val_f1)/float(len(val_f1)))\n",
    "print(sum(val_bin_cross)/float(len(val_bin_cross)))   #low is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = None\n",
    "# checkpoint_predictions = []\n",
    "# weights = []\n",
    "\n",
    "\n",
    "# #https://keras.io/callbacks/#learningratescheduler\n",
    "\n",
    "# for model_idx in range(NUM_MODELS):\n",
    "#   # build the same models\n",
    "#     model = build_model(embedding_matrix, y_aux_train.shape[-1])\n",
    "#   # We train each model EPOCHS times\n",
    "#   # After each epoch, we reset learning rate (we are using Adam Optimizer)  \n",
    "#   # https://towardsdatascience.com/learning-rate-scheduler-d8a55747dd90\n",
    "\n",
    "#   # https://github.com/keras-team/keras/blob/master/keras/callbacks.py#L921\n",
    "#   # learningrate is the attribute 'lr' from Adam optimizer\n",
    "#   # see https://github.com/keras-team/keras/blob/master/keras/optimizers.py#L460\n",
    "#   # In Adam Optimizer, learning rate is changing after each batch\n",
    "#     for global_epoch in range(EPOCHS):\n",
    "#         model.fit(\n",
    "#             x_train,\n",
    "#             [y_train, y_aux_train],\n",
    "#             batch_size=BATCH_SIZE,\n",
    "#             epochs=1,\n",
    "#             verbose=1,\n",
    "#             callbacks=[\n",
    "#                 #LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch), verbose = 1)\n",
    "#                 LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n",
    "#             ]\n",
    "#         )\n",
    "#         #model.predict will give two outputs: main_output (target) and aux_output\n",
    "#         #we only take main_output\n",
    "#         checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n",
    "#         weights.append(2 ** global_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = np.array([0,4,6])\n",
    "# (y_aux_train.iloc[indices,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory\n",
    "```\n",
    "Do i need to wait for the other gpu process to finish? this error message is suggesting that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate against test_size examples held out from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# continue original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "Weights sum to zero, can't be normalized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-c934c43359a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#take average (with weights) of predictions from two models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#predictions is an np.array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscl\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             raise ZeroDivisionError(\n\u001b[0;32m--> 422\u001b[0;31m                 \"Weights sum to zero, can't be normalized\")\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresult_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mscl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: Weights sum to zero, can't be normalized"
     ]
    }
   ],
   "source": [
    "#take average (with weights) of predictions from two models\n",
    "#predictions is an np.array\n",
    "predictions = np.average(checkpoint_predictions, weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test['id'],\n",
    "    'prediction': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  2 19:18:03 UTC 2019\r\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
